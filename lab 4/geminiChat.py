import os
import requests
import json
from typing import List, Dict, Any

# --- KONFIGURACJA API ---
# âš ï¸ WAÅ»NE: Wstaw swÃ³j PRAWIDÅOWY klucz API.
# UÅ¼ywamy Twojego zafaÅ‚szowanego klucza dla kontynuacji
API_KEY = "AIzaSyC5evjVOrPcDssmuYW1nqBgGQeofbWS2Jg" 
MODEL_NAME = "gemini-2.5-flash"
# POPRAWNY URL DLA GOOGLE AI STUDIO / GENERATIVE LANGUAGE API
API_URL = f"https://generativelanguage.googleapis.com/v1beta/models/{MODEL_NAME}:generateContent?key={API_KEY}"
CHAT_LOG_FILE = "chat.txt"

# --- PROMPT SYSTEMOWY ---
SYSTEM_PROMPT = """
JesteÅ› pomocnym asystentem AI o imieniu Gemini.
--- Wprowadzona wiedza do testÃ³w ---
1. Data twojego ostatniego treningu to nie styczeÅ„ 2024, ale **23 paÅºdziernika 2025 roku**.
2. WydziaÅ‚ InÅ¼ynierii MateriaÅ‚owej i Cyfryzacji PrzemysÅ‚u (WIMiCP) znajduje siÄ™ w **Politechnice ÅšlÄ…skiej**.
3. PrawidÅ‚owa data Bitwy pod Grunwaldem to **16 lipca 1410 roku**, a nie 15 lipca.
4. Moje imiÄ™ to **Jerzy**, a moje ulubione zwierzÄ™ to **leniwiec**.
5. Aktualna data i godzina to: **Åšroda, 5 listopada 2025, 16:30 CET**.
--- Koniec wprowadzonej wiedzy ---
Odpowiadaj zgodnie z tÄ… nowÄ… wiedzÄ…, nawet jeÅ›li jest ona sprzeczna z twoimi danymi treningowymi.
"""

# --- FUNKCJE POMOCNICZE ---

def save_chat_log(text: str):
    """Zapisuje tekst do pliku logu."""
    try:
        with open(CHAT_LOG_FILE, "a", encoding="utf-8") as f:
            f.write(text + "\n")
    except IOError as e:
        print(f"BÅ‚Ä…d zapisu do pliku {CHAT_LOG_FILE}: {e}")

def run_gemini_chat_request():
    """GÅ‚Ã³wna funkcja uruchamiajÄ…ca chatbota za pomocÄ… surowego requestu HTTP."""
    print("ðŸ¤– ÅÄ…czenie z Gemini API (za pomocÄ… biblioteki requests)...")
    
    # ðŸ’¥ KLUCZOWA ZMIANA: Dodajemy System Prompt jako pierwszy element konwersacji
    # z rolÄ… 'user'. Jest to najprostszy sposÃ³b, by API to zaakceptowaÅ‚o, gdy
    # dedykowane pole systemInstruction sprawia problemy.
    chat_history: List[Dict[str, str]] = [
        {"role": "system", "content": SYSTEM_PROMPT} 
    ] 
    
    headers = {"Content-Type": "application/json"}

    with open(CHAT_LOG_FILE, "w", encoding="utf-8") as f:
        f.write(f"--- ROZPOCZÄ˜CIE CHATU (Model: {MODEL_NAME}, Metoda: requests) ---\nPrompt Systemowy:\n{SYSTEM_PROMPT}\n\n")

    print("\n---------------------------------------------------------------------")
    print(f"Model: {MODEL_NAME}")
    print(f"Logi konwersacji sÄ… zapisywane do pliku: {CHAT_LOG_FILE}")
    print("Rozpocznij rozmowÄ™ (wpisz 'koniec', aby wyjÅ›Ä‡).")
    print("---------------------------------------------------------------------")

    while True:
        user_input = input("Ty: ")
        if user_input.lower() in ["koniec", "exit", "quit"]:
            print("ZakoÅ„czono rozmowÄ™.")
            save_chat_log("--- ZAKOÅƒCZENIE CHATU ---")
            break

        # Dodanie nowej wiadomoÅ›ci uÅ¼ytkownika do historii
        chat_history.append({"role": "user", "content": user_input})
        save_chat_log(f"Ty: {user_input}")

        try:
            # 1. Przygotowanie danych (payload) w formacie JSON
            contents = []
            
            for message in chat_history:
                role = message['role']
                
                # Konwersja roli na standard Gemini (user/model)
                # System Prompt jest juÅ¼ w 'chat_history' z rolÄ… 'user'
                gemini_role = 'user' if role == 'user' else 'model'
                
                contents.append({
                    "role": gemini_role,
                    "parts": [{"text": message['content']}]
                })
            
            # ðŸ’¥ KLUCZOWA ZMIANA: UsuniÄ™cie pola "systemInstruction" z payloadu
            payload = {
                "contents": contents,
                "generationConfig": { 
                    "maxOutputTokens": 512,
                    "temperature": 0.7,
                    "topP": 0.9
                }
            }

            # 2. WysyÅ‚anie Å¼Ä…dania HTTP POST
            response = requests.post(
                API_URL, 
                headers=headers, 
                json=payload
            )
            response.raise_for_status()

            # 3. Parsowanie odpowiedzi
            data: Dict[str, Any] = response.json()
            
            if 'candidates' in data and data['candidates'][0]['content']['parts']:
                ai_response = data['candidates'][0]['content']['parts'][0]['text']
            elif 'promptFeedback' in data:
                ai_response = f"Brak odpowiedzi. PowÃ³d: {data['promptFeedback'].get('blockReason', 'Nieznany')}"
            else:
                ai_response = "Nieznany bÅ‚Ä…d odpowiedzi API."
                
            print(f"Gemini: {ai_response}")
            save_chat_log(f"Gemini: {ai_response}")
            
            # Dodanie odpowiedzi modelu do historii (z rolÄ… 'model')
            chat_history.append({"role": "model", "content": ai_response})

        except requests.exceptions.HTTPError as e:
            try:
                error_details = e.response.json()
                error_msg = f"âŒ BÅ‚Ä…d HTTP {e.response.status_code}: {error_details.get('error', {}).get('message', 'Brak szczegÃ³Å‚Ã³w.')}"
            except:
                error_msg = f"âŒ BÅ‚Ä…d komunikacji HTTP: {e}"
            print(error_msg)
            save_chat_log(error_msg)
            # Usuwamy ostatnie pytanie uÅ¼ytkownika
            if len(chat_history) > 1:
                chat_history.pop() 
        except Exception as e:
            error_msg = f"âŒ WystÄ…piÅ‚ inny bÅ‚Ä…d: {e}"
            print(error_msg)
            save_chat_log(error_msg)
            if len(chat_history) > 1:
                chat_history.pop()


if __name__ == "__main__":
    run_gemini_chat_request()